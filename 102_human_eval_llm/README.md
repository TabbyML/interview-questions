# HumanEval on LLM

This project is to collect the performance of language model on HumanEval and display the result in a web UI.

# Instructions

This project contains 2 parts, one is to collect the HumanEval result, one is to display the result. We recommend you to first mock the result and finish the UI part.
If time permits, you can use [HumanEval](http://github.com/openai/human_eval) to obtain the real benchmark for a model listed in [LLM Models](https://github.com/wsxiaoys/awesome-ai-coding#llm-models).

# Minimum Requirements

1. A UI to display the result.
   - If you mock the result, we require the UI to show the HumanEval for multiple models.
   - If you collect the actual result, we only require the UI to show the result for a single language model.

# Good to have

### Tech Stack

We don't enforce the tech stack in the assignment, but we recommend the tech stack used in our main product. since that's the one you will work with after join. So you may try to get a sense beforehand.

1. Frontend - TypeScript, React, Tailwind CSS.
2. Backend - Next.JS. It's also totally fine if your solution doesn't involve backend at all.

### Features

We don't require any additional features. If you have more time, we recommend you to polish those minimum requirements. But if you can't resist the temptation, just go ahead and surprise us.

# Deliverables

Instruction on how to download the code and run locally.
